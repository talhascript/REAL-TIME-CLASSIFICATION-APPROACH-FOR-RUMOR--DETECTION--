{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "v5kHSU9prFIe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ac4c6833-ba92-4cbf-86be-1c7fd75772dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting rdflib\n",
            "  Downloading rdflib-7.0.0-py3-none-any.whl (531 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/531.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━\u001b[0m\u001b[90m╺\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m174.1/531.9 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m531.9/531.9 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting isodate<0.7.0,>=0.6.0 (from rdflib)\n",
            "  Downloading isodate-0.6.1-py2.py3-none-any.whl (41 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.7 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Installing collected packages: isodate, rdflib\n",
            "Successfully installed isodate-0.6.1 rdflib-7.0.0\n",
            "Epoch 1/10\n",
            "572/572 [==============================] - 12s 18ms/step - loss: 0.4207 - accuracy: 0.8236\n",
            "Epoch 2/10\n",
            "572/572 [==============================] - 11s 20ms/step - loss: 0.2719 - accuracy: 0.8934\n",
            "Epoch 3/10\n",
            "572/572 [==============================] - 12s 20ms/step - loss: 0.1980 - accuracy: 0.9223\n",
            "Epoch 4/10\n",
            "572/572 [==============================] - 11s 20ms/step - loss: 0.1177 - accuracy: 0.9579\n",
            "Epoch 5/10\n",
            "572/572 [==============================] - 11s 19ms/step - loss: 0.0679 - accuracy: 0.9767\n",
            "Epoch 6/10\n",
            "572/572 [==============================] - 10s 17ms/step - loss: 0.0440 - accuracy: 0.9860\n",
            "Epoch 7/10\n",
            "572/572 [==============================] - 12s 20ms/step - loss: 0.0335 - accuracy: 0.9892\n",
            "Epoch 8/10\n",
            "572/572 [==============================] - 11s 20ms/step - loss: 0.0259 - accuracy: 0.9902\n",
            "Epoch 9/10\n",
            "572/572 [==============================] - 11s 20ms/step - loss: 0.0224 - accuracy: 0.9902\n",
            "Epoch 10/10\n",
            "572/572 [==============================] - 10s 18ms/step - loss: 0.0208 - accuracy: 0.9899\n",
            "143/143 [==============================] - 1s 5ms/step\n",
            "Accuracy: 0.8484914735461303\n",
            "Precision: 0.8816405174786678\n",
            "True Positives: 678\n",
            "False Negatives: 430\n",
            "False Positives: 263\n",
            "True Negatives: 3203\n"
          ]
        }
      ],
      "source": [
        "!pip install rdflib\n",
        "\n",
        "import rdflib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load RDF graph\n",
        "g = rdflib.Graph()\n",
        "g.parse(\"/content/populated_ontology.ttl\", format=\"ttl\")\n",
        "\n",
        "# Scan the graph to extract Article and Source information\n",
        "articles = {}\n",
        "for s in g.subjects(rdflib.RDF.type, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/Article\")):\n",
        "    title = str(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/title\")))\n",
        "    tweet_count = int(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/tweetCount\")))\n",
        "    is_real = int(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/isReal\")))\n",
        "    source = g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/publishedBy\"))\n",
        "    source_domain = str(g.value(source, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/sourceDomain\")))\n",
        "\n",
        "    articles[s] = {'title': title, 'tweet_count': tweet_count, 'is_real': is_real, 'source_domain': source_domain}\n",
        "\n",
        "# Convert dictionary to DataFrame\n",
        "df = pd.DataFrame.from_dict(articles, orient='index')\n",
        "\n",
        "# Text data preprocessing\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['title'])\n",
        "title_sequences = tokenizer.texts_to_sequences(df['title'])\n",
        "title_data = pad_sequences(title_sequences, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(df['source_domain'])\n",
        "source_sequences = tokenizer.texts_to_sequences(df['source_domain'])\n",
        "source_data = pad_sequences(source_sequences, maxlen=10)\n",
        "\n",
        "# Normalizing tweet counts\n",
        "tweet_counts_normalized = np.array(df['tweet_count'])\n",
        "tweet_counts_normalized = (tweet_counts_normalized - np.mean(tweet_counts_normalized)) / np.std(tweet_counts_normalized)\n",
        "tweet_counts_normalized = tweet_counts_normalized.reshape(-1, 1)  # Reshape for horizontal stacking\n",
        "\n",
        "# Preparing data and labels\n",
        "X = np.hstack((title_data, source_data, tweet_counts_normalized))\n",
        "y = df['is_real'].values\n",
        "\n",
        "# Check the total length of input features\n",
        "input_length = X.shape[1]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=64, input_length=input_length),  # Correct input_length based on actual data width\n",
        "    Conv1D(64, 5, activation='relu'),\n",
        "    MaxPooling1D(5),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "tp, fn, fp, tn = conf_matrix.ravel()\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"True Negatives: {tn}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install rdflib\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "import rdflib\n",
        "\n",
        "# Load RDF graph\n",
        "g = rdflib.Graph()\n",
        "g.parse(\"/content/populated_ontology.ttl\", format=\"ttl\")\n",
        "\n",
        "# Scan the graph to extract Article information\n",
        "articles = []\n",
        "for s in g.subjects(rdflib.RDF.type, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/Article\")):\n",
        "    title = str(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/title\")))\n",
        "    articles.append(title)\n",
        "\n",
        "# Convert list of articles to a corpus\n",
        "corpus = articles\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(corpus)\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Print the top terms per cluster\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    for ind in order_centroids[i, :10]:  # Print top 10 terms per cluster\n",
        "        print(f\"{terms[ind]}\")\n",
        "    print()\n",
        "\n",
        "# Assign each article to a cluster\n",
        "article_clusters = kmeans.predict(X)\n",
        "\n",
        "# Count the number of articles in each cluster\n",
        "cluster_counts = {}\n",
        "for cluster in article_clusters:\n",
        "    if cluster not in cluster_counts:\n",
        "        cluster_counts[cluster] = 1\n",
        "    else:\n",
        "        cluster_counts[cluster] += 1\n",
        "\n",
        "# Print the count of articles in each cluster\n",
        "for cluster, count in cluster_counts.items():\n",
        "    print(f\"Cluster {cluster}: {count} Articles\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hDy1uk0RtN7x",
        "outputId": "d88dee00-4a11-4c0c-b4a1-46cd90c3ead9"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: rdflib in /usr/local/lib/python3.10/dist-packages (7.0.0)\n",
            "Requirement already satisfied: isodate<0.7.0,>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (0.6.1)\n",
            "Requirement already satisfied: pyparsing<4,>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from rdflib) (3.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.10/dist-packages (from isodate<0.7.0,>=0.6.0->rdflib) (1.16.0)\n",
            "Cluster 0:\n",
            "meghan\n",
            "markle\n",
            "prince\n",
            "harry\n",
            "royal\n",
            "wedding\n",
            "middleton\n",
            "kate\n",
            "william\n",
            "queen\n",
            "\n",
            "Cluster 1:\n",
            "2018\n",
            "awards\n",
            "2017\n",
            "music\n",
            "list\n",
            "choice\n",
            "winners\n",
            "tv\n",
            "carpet\n",
            "best\n",
            "\n",
            "Cluster 2:\n",
            "season\n",
            "renewed\n",
            "premiere\n",
            "finale\n",
            "cast\n",
            "trailer\n",
            "new\n",
            "13\n",
            "episode\n",
            "little\n",
            "\n",
            "Cluster 3:\n",
            "kardashian\n",
            "kim\n",
            "khloe\n",
            "kourtney\n",
            "west\n",
            "kanye\n",
            "thompson\n",
            "tristan\n",
            "jenner\n",
            "baby\n",
            "\n",
            "Cluster 4:\n",
            "new\n",
            "jennifer\n",
            "justin\n",
            "jenner\n",
            "baby\n",
            "star\n",
            "brad\n",
            "says\n",
            "pitt\n",
            "selena\n",
            "\n",
            "Cluster 4: 19331 Articles\n",
            "Cluster 1: 1121 Articles\n",
            "Cluster 3: 1159 Articles\n",
            "Cluster 2: 630 Articles\n",
            "Cluster 0: 625 Articles\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cluster_names = {\n",
        "    0: \"Royal News\",\n",
        "    1: \"Entertainment Awards\",\n",
        "    2: \"TV Show and Series\",\n",
        "    3: \"Celebrity Gossip (Kardashians)\",\n",
        "    4: \"Celebrity Relationships and Gossip\"\n",
        "}\n",
        "\n",
        "# Print the renamed clusters\n",
        "for cluster, name in cluster_names.items():\n",
        "    print(f\"{name}: {cluster_counts[cluster]} Articles\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uOozz3_s27pK",
        "outputId": "90a1061e-5712-4aab-bd81-1675816ce15b"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Royal News: 625 Articles\n",
            "Entertainment Awards: 1121 Articles\n",
            "TV Show and Series: 630 Articles\n",
            "Celebrity Gossip (Kardashians): 1159 Articles\n",
            "Celebrity Relationships and Gossip: 19331 Articles\n"
          ]
        }
      ]
    }
  ]
}