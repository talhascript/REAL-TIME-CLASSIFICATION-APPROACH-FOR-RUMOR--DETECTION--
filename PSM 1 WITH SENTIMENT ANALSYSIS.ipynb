{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zGnzTH6v92i4",
        "outputId": "2b44d203-9a6a-4f80-f901-44fa4b9bab4b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Fold 1\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 4s 9ms/step - loss: 0.4908 - accuracy: 0.7850\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.3368 - accuracy: 0.8577\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 4s 13ms/step - loss: 0.2745 - accuracy: 0.8840\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 3s 10ms/step - loss: 0.2273 - accuracy: 0.9047\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.1855 - accuracy: 0.9277\n",
            "143/143 [==============================] - 0s 2ms/step\n",
            "True Positives: 3139\n",
            "True Negatives: 633\n",
            "False Positives: 472\n",
            "False Negatives: 330\n",
            "Accuracy: 82.47%\n",
            "Precision: 86.93%\n",
            "FINISH\n",
            "Fold 2\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.4854 - accuracy: 0.7866\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 4s 14ms/step - loss: 0.3365 - accuracy: 0.8573\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2765 - accuracy: 0.8861\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2312 - accuracy: 0.9045\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.1924 - accuracy: 0.9219\n",
            "143/143 [==============================] - 0s 2ms/step\n",
            "True Positives: 3162\n",
            "True Negatives: 646\n",
            "False Positives: 493\n",
            "False Negatives: 272\n",
            "Accuracy: 83.27%\n",
            "Precision: 86.51%\n",
            "FINISH\n",
            "Fold 3\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 5s 14ms/step - loss: 0.4875 - accuracy: 0.7862\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.3372 - accuracy: 0.8568\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2759 - accuracy: 0.8859\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2329 - accuracy: 0.9047\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 3s 10ms/step - loss: 0.1941 - accuracy: 0.9231\n",
            "143/143 [==============================] - 0s 2ms/step\n",
            "True Positives: 3176\n",
            "True Negatives: 627\n",
            "False Positives: 442\n",
            "False Negatives: 328\n",
            "Accuracy: 83.16%\n",
            "Precision: 87.78%\n",
            "FINISH\n",
            "Fold 4\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 4s 10ms/step - loss: 0.4864 - accuracy: 0.7902\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 4s 14ms/step - loss: 0.3383 - accuracy: 0.8576\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2808 - accuracy: 0.8832\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2402 - accuracy: 0.9027\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2047 - accuracy: 0.9189\n",
            "143/143 [==============================] - 0s 2ms/step\n",
            "True Positives: 3184\n",
            "True Negatives: 620\n",
            "False Positives: 458\n",
            "False Negatives: 311\n",
            "Accuracy: 83.18%\n",
            "Precision: 87.42%\n",
            "FINISH\n",
            "Fold 5\n",
            "Epoch 1/5\n",
            "286/286 [==============================] - 5s 14ms/step - loss: 0.4811 - accuracy: 0.7883\n",
            "Epoch 2/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.3333 - accuracy: 0.8583\n",
            "Epoch 3/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2728 - accuracy: 0.8880\n",
            "Epoch 4/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.2287 - accuracy: 0.9061\n",
            "Epoch 5/5\n",
            "286/286 [==============================] - 3s 9ms/step - loss: 0.1896 - accuracy: 0.9248\n",
            "143/143 [==============================] - 1s 3ms/step\n",
            "True Positives: 3156\n",
            "True Negatives: 617\n",
            "False Positives: 487\n",
            "False Negatives: 313\n",
            "Accuracy: 82.51%\n",
            "Precision: 86.63%\n",
            "FINISH\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score,precision_score\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D,GlobalMaxPooling1D, Dense\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'FakeNewsNet.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "# Data Preprocessing\n",
        "df = df.dropna()\n",
        "X = df['title'].values\n",
        "y = df['real'].values\n",
        "# K-fold cross-validation\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "for fold, (train_idx, test_idx) in enumerate(kf.split(X)):\n",
        "  print(f\"Fold {fold + 1}\")\n",
        "  # Split the dataset into train and test sets for this fold\n",
        "  X_train, X_test = X[train_idx], X[test_idx]\n",
        "  y_train, y_test = y[train_idx], y[test_idx]\n",
        "  # Tokenize and pad text data for training\n",
        "  tokenizer = Tokenizer(num_words=5000, oov_token='<OOV>')\n",
        "  tokenizer.fit_on_texts(X_train)\n",
        "  X_train_sequences = tokenizer.texts_to_sequences(X_train)\n",
        "  X_train_padded = pad_sequences(X_train_sequences, maxlen=50,\n",
        "  padding='post', truncating='post')\n",
        "  # Define and compile the model\n",
        "  model = Sequential()\n",
        "  model.add(Embedding(input_dim=5000, output_dim=16,\n",
        "  input_length=50))\n",
        "  model.add(Conv1D(128, 5, activation='relu'))\n",
        "  model.add(GlobalMaxPooling1D())\n",
        "  model.add(Dense(1, activation='sigmoid'))\n",
        "  model.compile(optimizer='adam', loss='binary_crossentropy',\n",
        "  metrics=['accuracy'])\n",
        "  # Train the model on the current fold\n",
        "  model.fit(X_train_padded, y_train, epochs=5, batch_size=64,\n",
        "  verbose=1)\n",
        "  # Tokenize and pad text data for testing\n",
        "  X_test_sequences = tokenizer.texts_to_sequences(X_test)\n",
        "  X_test_padded = pad_sequences(X_test_sequences, maxlen=50,\n",
        "  padding='post', truncating='post')\n",
        "  # Predictions and Confusion Matrix\n",
        "  y_pred = (model.predict(X_test_padded) > 0.5).astype('int')\n",
        "  conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "  # Extract values from the confusion matrix\n",
        "  tn, fp, fn, tp = conf_matrix.ravel()\n",
        "  # Print True Positives, True Negatives, False Positives, and False Negatives\n",
        "  print(f'True Positives: {tp}')\n",
        "  print(f'True Negatives: {tn}')\n",
        "  print(f'False Positives: {fp}')\n",
        "  print(f'False Negatives: {fn}')\n",
        "  # Calculate and print evaluation metrics\n",
        "  accuracy = accuracy_score(y_test, y_pred)\n",
        "  precision = precision_score(y_test, y_pred)\n",
        "  print(f'Accuracy: {accuracy * 100:.2f}%')\n",
        "  print(f'Precision: {precision * 100:.2f}%')\n",
        "  print('FINISH')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqDT7FXhBDgL",
        "outputId": "b5b345a5-1223-4c20-c6a4-28f7e4fb144b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sklearn/cluster/_kmeans.py:870: FutureWarning: The default value of `n_init` will change from 10 to 'auto' in 1.4. Set the value of `n_init` explicitly to suppress the warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Cluster 0:\n",
            "new\n",
            "2018\n",
            "season\n",
            "awards\n",
            "jenner\n",
            "jennifer\n",
            "star\n",
            "baby\n",
            "says\n",
            "best\n",
            "\n",
            "Cluster 1:\n",
            "kardashian\n",
            "kim\n",
            "khloe\n",
            "kourtney\n",
            "west\n",
            "kanye\n",
            "thompson\n",
            "tristan\n",
            "jenner\n",
            "baby\n",
            "\n",
            "Cluster 2:\n",
            "selena\n",
            "gomez\n",
            "bieber\n",
            "justin\n",
            "weeknd\n",
            "relationship\n",
            "timeline\n",
            "complete\n",
            "hailey\n",
            "theroux\n",
            "\n",
            "Cluster 3:\n",
            "brad\n",
            "pitt\n",
            "angelina\n",
            "jolie\n",
            "aniston\n",
            "jennifer\n",
            "divorce\n",
            "kids\n",
            "dating\n",
            "custody\n",
            "\n",
            "Cluster 4:\n",
            "meghan\n",
            "markle\n",
            "prince\n",
            "harry\n",
            "royal\n",
            "wedding\n",
            "middleton\n",
            "kate\n",
            "william\n",
            "queen\n",
            "\n",
            "Cluster 0: 20101 Articles\n",
            "Cluster 3: 524 Articles\n",
            "Cluster 4: 625 Articles\n",
            "Cluster 1: 1165 Articles\n",
            "Cluster 2: 451 Articles\n",
            "Royal News: 20101 Articles\n",
            "Entertainment Awards: 1165 Articles\n",
            "TV Show and Series: 451 Articles\n",
            "Celebrity Gossip (Kardashians): 524 Articles\n",
            "Celebrity Relationships and Gossip: 625 Articles\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.cluster import KMeans\n",
        "\n",
        "# Load the dataset\n",
        "file_path = 'FakeNewsNet.csv'\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Data Preprocessing\n",
        "df = df.dropna()\n",
        "titles = df['title'].values\n",
        "\n",
        "# TF-IDF vectorization\n",
        "vectorizer = TfidfVectorizer(stop_words='english')\n",
        "X = vectorizer.fit_transform(titles)\n",
        "\n",
        "# Apply K-means clustering\n",
        "num_clusters = 5  # Adjust the number of clusters as needed\n",
        "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
        "kmeans.fit(X)\n",
        "\n",
        "# Print the top terms per cluster\n",
        "order_centroids = kmeans.cluster_centers_.argsort()[:, ::-1]\n",
        "terms = vectorizer.get_feature_names_out()\n",
        "for i in range(num_clusters):\n",
        "    print(f\"Cluster {i}:\")\n",
        "    for ind in order_centroids[i, :10]:  # Print top 10 terms per cluster\n",
        "        print(f\"{terms[ind]}\")\n",
        "    print()\n",
        "\n",
        "# Assign each article to a cluster\n",
        "article_clusters = kmeans.predict(X)\n",
        "\n",
        "# Count the number of articles in each cluster\n",
        "cluster_counts = {}\n",
        "for cluster in article_clusters:\n",
        "    if cluster not in cluster_counts:\n",
        "        cluster_counts[cluster] = 1\n",
        "    else:\n",
        "        cluster_counts[cluster] += 1\n",
        "\n",
        "# Print the count of articles in each cluster\n",
        "for cluster, count in cluster_counts.items():\n",
        "    print(f\"Cluster {cluster}: {count} Articles\")\n",
        "\n",
        "# Rename clusters\n",
        "cluster_names = {\n",
        "    0: \"Royal News\",\n",
        "    1: \"Entertainment Awards\",\n",
        "    2: \"TV Show and Series\",\n",
        "    3: \"Celebrity Gossip (Kardashians)\",\n",
        "    4: \"Celebrity Relationships and Gossip\"\n",
        "}\n",
        "\n",
        "# Print the renamed clusters\n",
        "for cluster, name in cluster_names.items():\n",
        "    print(f\"{name}: {cluster_counts[cluster]} Articles\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
