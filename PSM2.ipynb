{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 48,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u295hh4e9Ez8",
        "outputId": "be7a4e86-e0cc-49af-e2bd-449de8f44b24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "572/572 [==============================] - 14s 23ms/step - loss: 0.4130 - accuracy: 0.8284\n",
            "Epoch 2/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.2690 - accuracy: 0.8941\n",
            "Epoch 3/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.1924 - accuracy: 0.9246\n",
            "Epoch 4/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.1132 - accuracy: 0.9599\n",
            "Epoch 5/10\n",
            "572/572 [==============================] - 12s 22ms/step - loss: 0.0639 - accuracy: 0.9796\n",
            "Epoch 6/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.0446 - accuracy: 0.9859\n",
            "Epoch 7/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.0327 - accuracy: 0.9881\n",
            "Epoch 8/10\n",
            "572/572 [==============================] - 13s 22ms/step - loss: 0.0244 - accuracy: 0.9900\n",
            "Epoch 9/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.0206 - accuracy: 0.9911\n",
            "Epoch 10/10\n",
            "572/572 [==============================] - 13s 23ms/step - loss: 0.0201 - accuracy: 0.9899\n",
            "143/143 [==============================] - 1s 6ms/step\n",
            "Accuracy: 0.830126803672934\n",
            "Precision: 0.8960235640648012\n",
            "True Positives: 755\n",
            "False Negatives: 353\n",
            "False Positives: 424\n",
            "True Negatives: 3042\n"
          ]
        }
      ],
      "source": [
        "import rdflib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Embedding, Conv1D, MaxPooling1D, Dense, Flatten, Dropout\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from sklearn.metrics import accuracy_score, precision_score, confusion_matrix\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Load RDF graph\n",
        "g = rdflib.Graph()\n",
        "g.parse(\"/content/populated_ontology.ttl\", format=\"ttl\")\n",
        "\n",
        "# Scan the graph to extract Article and Source information\n",
        "articles = {}\n",
        "for s in g.subjects(rdflib.RDF.type, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/Article\")):\n",
        "    title = str(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/title\")))\n",
        "    tweet_count = int(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/tweetCount\")))\n",
        "    is_real = int(g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/isReal\")))\n",
        "    source = g.value(s, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/publishedBy\"))\n",
        "    source_domain = str(g.value(source, rdflib.URIRef(\"http://www.semanticweb.org/talha/ontologies/2024/3/rumour_detection/sourceDomain\")))\n",
        "\n",
        "    articles[s] = {'title': title, 'tweet_count': tweet_count, 'is_real': is_real, 'source_domain': source_domain}\n",
        "\n",
        "# Convert dictionary to DataFrame\n",
        "df = pd.DataFrame.from_dict(articles, orient='index')\n",
        "\n",
        "# Text data preprocessing\n",
        "tokenizer = Tokenizer(num_words=5000)\n",
        "tokenizer.fit_on_texts(df['title'])\n",
        "title_sequences = tokenizer.texts_to_sequences(df['title'])\n",
        "title_data = pad_sequences(title_sequences, maxlen=100)\n",
        "\n",
        "tokenizer.fit_on_texts(df['source_domain'])\n",
        "source_sequences = tokenizer.texts_to_sequences(df['source_domain'])\n",
        "source_data = pad_sequences(source_sequences, maxlen=10)\n",
        "\n",
        "# Normalizing tweet counts\n",
        "tweet_counts_normalized = np.array(df['tweet_count'])\n",
        "tweet_counts_normalized = (tweet_counts_normalized - np.mean(tweet_counts_normalized)) / np.std(tweet_counts_normalized)\n",
        "tweet_counts_normalized = tweet_counts_normalized.reshape(-1, 1)  # Reshape for horizontal stacking\n",
        "\n",
        "# Preparing data and labels\n",
        "X = np.hstack((title_data, source_data, tweet_counts_normalized))\n",
        "y = df['is_real'].values\n",
        "\n",
        "# Check the total length of input features\n",
        "input_length = X.shape[1]\n",
        "\n",
        "# Split data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# CNN Model\n",
        "model = Sequential([\n",
        "    Embedding(input_dim=5000, output_dim=64, input_length=input_length),  # Correct input_length based on actual data width\n",
        "    Conv1D(64, 5, activation='relu'),\n",
        "    MaxPooling1D(5),\n",
        "    Flatten(),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dropout(0.5),\n",
        "    Dense(1, activation='sigmoid')\n",
        "])\n",
        "\n",
        "model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "# Train model\n",
        "model.fit(X_train, y_train, epochs=10, batch_size=32)\n",
        "\n",
        "# Evaluate model\n",
        "y_pred = (model.predict(X_test) > 0.5).astype(\"int32\")\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "tp, fn, fp, tn = conf_matrix.ravel()\n",
        "\n",
        "print(f\"Accuracy: {accuracy}\")\n",
        "print(f\"Precision: {precision}\")\n",
        "print(f\"True Positives: {tp}\")\n",
        "print(f\"False Negatives: {fn}\")\n",
        "print(f\"False Positives: {fp}\")\n",
        "print(f\"True Negatives: {tn}\")\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
